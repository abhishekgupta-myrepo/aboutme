{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#home","title":"Home","text":""},{"location":"#section-artificial-intelligence","title":"Section-Artificial Intelligence","text":""},{"location":"#section-enterprise-information-management","title":"Section-Enterprise Information Management","text":""},{"location":"#chapter-data-management","title":"Chapter-Data Management","text":"<ul> <li>Snowflake versus Databricks</li> </ul>"},{"location":"#chapter-containers-and-kubernetes","title":"Chapter-Containers and Kubernetes","text":"<ul> <li>About Docker</li> <li>CI with Travis</li> <li>Docker Compose</li> <li>Docker-Commands</li> <li>Docker-Swarm</li> <li>Kubernetes</li> <li>Simple Docker Project</li> </ul>"},{"location":"#section-healthcare","title":"Section-Healthcare","text":""},{"location":"#section-management","title":"Section-Management","text":""},{"location":"Abhishek/About%20me/","title":"About me","text":"<p>hi</p>"},{"location":"Artificial%20Intelligence/Generative%20AI%20with%20Large%20Language%20Models/","title":"Generative AI with Large Language Models","text":""},{"location":"Artificial%20Intelligence/Generative%20AI%20with%20Large%20Language%20Models/#generative-ai-with-large-language-models","title":"Generative AI with Large Language Models","text":""},{"location":"Artificial%20Intelligence/Generative%20AI%20with%20Large%20Language%20Models/#what-pattern-is-similar-between-openai-chatgpt-and-tesla-model-3","title":"What pattern is similar between OpenAI: ChatGPT and Tesla Model 3","text":"<p>The underlying technology in both was thrust into public consciousness only after the release of these products, although the technology (Transformers in case of ChatGPT: Electric motor for cars in case of Model 3) existed for many years earlier.</p> <ul> <li>Generative AI with LLMs is a general purpose technology that can be applied to many use cases. This is a new technology with a lot of work to be done in the area. Just like deep learning that originated 15 years back, it has huge growth potential.</li> </ul>"},{"location":"Artificial%20Intelligence/Generative%20AI%20with%20Large%20Language%20Models/#terms-used-in-large-language-models","title":"Terms used in Large Language Models","text":"<ul> <li>Training generative AI models differs from conventional models as it involves prompts and completion, known as inference.</li> <li>Prompts usually consist of context windows with 1000 words or fewer. However, there are models from Anthropic that provide very large context windows.</li> <li>Completion refers to the model's output, which predicts the next word based on the input, and this process is called inference.</li> <li>In the context of context learning, where the desired model output is provided as part of the prompt, there are three key ways to approach it:<ul> <li>Zero-shot inference: In this approach, no specific examples of the desired output are given to the model.</li> <li>One-shot inference: Here, the model is directed on what to do and provided with an example of the desired output. This increases the chances of accurate output generation, even with smaller models.</li> <li>Few-shot inference: Multiple examples with different scenarios are included, assisting in generating output even with smaller models.</li> </ul> </li> <li>Prompts play a vital role in generating the desired output. Larger-sized models excel at zero-shot inference, while smaller models are efficient for a narrower set of tasks.</li> <li>However, it's crucial to consider the context window, which limits the amount of data that can be used as a prompt.</li> <li>To enhance performance, fine-tuning can be employed by providing additional data, allowing the model to learn more effectively.</li> </ul>"},{"location":"Artificial%20Intelligence/Generative%20AI%20with%20Large%20Language%20Models/#legacy-ai-work","title":"Legacy AI work","text":"<ul> <li>RNN (Recurrent Neural Network) is a type of neural network that handles sequential data by introducing feedback loops. These loops allow RNNs to maintain a hidden state, capturing context over time. They are used in tasks like language translation, speech recognition, and more. Variants like LSTM and GRU address long-term dependency issues, making RNNs effective for modeling sequential data.</li> <li>Word2Vec is an NLP technique that creates word embeddings, converting words into meaningful numerical vectors. It represents words in a continuous vector space, capturing semantic relationships between words. These embeddings are widely used in various NLP tasks to enhance models' understanding of text and improve performance.</li> </ul>"},{"location":"Artificial%20Intelligence/Generative%20AI%20with%20Large%20Language%20Models/#models-tuning","title":"Models Tuning","text":"<ul> <li>the inference parameters of the model can be used to control the output behavior od the model and the inference it is making from the prompt. This is different from teh training pratamers.<ul> <li>max tokens - number of tokens that are generaed. It is the max of new toekns. It can very well happen that hte end of sequence is reached before the hard limit is reached.</li> <li>Greedy decoding - short generation - repeated sequence of words, natural output and more general - use the random sampling. In this case the model will choose the word based on teh randow weighted distribution. but in case of the random smapling is uncontrolled there is a risk of the model wnadering off to different words or topics.</li> <li>Hugging face - requires explicit selection of the sampling method. And will need you to explicitly disable the random sampling using the flag.</li> <li>This is contrilled using top K - more reasonable and makes more sense where the model restricts the weighted random sampling to the top k words only. Ensures some variability.</li> <li>Top p - is the number of samples whose some of the probabilities some up to be less than equal to &lt;=p</li> <li>Temperature - shape of the model probability distribution . This is applied at the final layer of the softmax function and controls the distribution of the probabilities. Lower temperatures &lt; 1 will result in strongly peaked distrbution resulting in odd favouring few words and therfore the output prediction will be less random. The hotter temperatures &gt; 1 will result in broader, flatter distribution this will result in some degree of randomess in the predicted word output duringt the random sampling. This will lead to more creative output.</li> <li>softmax will be used as default if the temp = 1 .</li> </ul> </li> </ul>"},{"location":"Artificial%20Intelligence/Generative%20AI%20with%20Large%20Language%20Models/#foundational-models","title":"Foundational Models","text":"<ul> <li>These foundational models comprise billions of parameters. Parameters are kind of the memory of the model. Larger the model the more sophisticated model becomes.</li> <li>The larger the model the more subjectiveness and understanding of the model is achieved that helps it to reason better. but it is not always true. for smaller and specific tasks smaller models tend to perform equally well by doing fine tuning.</li> <li>Timeline of AI and language models \u2013 Dr Alan D. Thompson \u2013 Life Architect</li> <li>Hugging Face</li> <li>BERT - 110M</li> <li>GPT</li> <li>FLAN-T5</li> <li>LLaMa</li> <li>PaLM</li> <li>BLOOM - 176B</li> </ul>"},{"location":"Artificial%20Intelligence/Generative%20AI%20with%20Large%20Language%20Models/#appropriateness-of-the-models","title":"Appropriateness of the models","text":"<ul> <li>It's not necessary to use the large parameter for every use case. Some single application use cases work well with smaller models. Therefore, with so many foundational models that are available it is important for the developers to learn which model will work the best and in which scenario it is best to build a new model vs. in which case fine-tuning existing model will make more sense.</li> <li>Pre-training the model from scratch  - self - supervised learning - model learns the pattern from the language that has been fed into it and the trains itself based on the objective function to minimize. The model is also dependent upon the architecture it selects.</li> <li>create embeddings / vector representations</li> <li>The encoder models  / auto-encoder models - objective is reconstruct text(\"denoising\") it is training using masked language modeling (MLM)  - build bi-directional understanding of the sequence. sentence level tasks such as sentence classification, sentiment, token level tasks - name entity recognition or word classification - BERT, ROBERT</li> <li>De-coder only models - auto-regressive models - uses causal language modeling (CLM) where the model tasks is to predict the next word. These are used in text generation and other general tasks - GPT , BLOOM. Show zero-shot inference capabilities.</li> <li>Encoder/Decoder models - sequence-to-sequence models - example - T5  and BART. uses span correction. This is useful in text summarization, translation, question answering. the sentences of variable length are masked and are then replaced with a snetinel token that does not belong to the dictionary and then uses the auto-regressive model to predict the next word to reconstruct the sentinel token.</li> <li>Some of the pre-trained models it has been observed that the larger the model the better it is at performing the general task. This has led to the growth of larger models. This has been fueled by the <ul> <li>availability of the transformer architecture that is highly scalable</li> <li>available of the data</li> <li>availability of the compute resources to train the mode. </li> </ul> </li> <li>Training these large model is super expensive and at a given point in time it becomes infeasible to train such models further.</li> </ul>"},{"location":"Artificial%20Intelligence/Generative%20AI%20with%20Large%20Language%20Models/#model-training-challenges","title":"Model Training Challenges","text":"<ul> <li>CUDA - Compute unified Device Architecture - CUDA out of memory - this is often recieved when using Nvidia GPUs for training or loading the models.</li> <li>Nvidia A100  - GPU models, 80GB ram is the max</li> <li>1B param model @32bit precision requires 80GB memory, @16bit require 40GB of memory, and @8bit require require 20GB of memory. so to fit the model you should use 16bit or 8 bit quantization.</li> <li>Quantization - FP32 --&gt; FP16 or BFLOT16 ( google , hybrid, supported by nvidia)</li> <li>![[Pasted image 20230717230341.png]]</li> </ul> <p>![[Pasted image 20230718223826.png]]</p> <p>![[Pasted image 20230718223842.png]]</p> <ul> <li>FSDP - ZeRO - Sharding based approach - MS paper</li> <li>Full Replication</li> <li>DDP - Distribute data parallel</li> <li>Full sharding</li> <li>Hybrid Sharding - beyong </li> <li>beyond 2.8B parameters DDP and full replication does not work.</li> </ul>"},{"location":"Artificial%20Intelligence/Generative%20AI%20with%20Large%20Language%20Models/#scaling-laws-and-compute-optimal-performance","title":"Scaling laws and compute optimal performance","text":"<ul> <li>kaplan - 2020 - scaling laws for neural language models</li> <li>Chinchilla paper - 2022 - training compute optimal large language model ![[Pasted image 20230718225304.png]]</li> <li>Metric to measure the compute budget required - <ul> <li>1 petaflop/S-days - floating point operations performed at the rate of 1 petaflop per second for 1 day. = 8 Nvidia V100s GPUs or 2 Nvidia A100 GPUs at 100% efficiency. ![[Pasted image 20230718230906.png]] [2303.17564] BloombergGPT: A Large Language Model for Finance (arxiv.org)</li> </ul> </li> </ul>"},{"location":"Artificial%20Intelligence/Generative%20AI%20with%20Large%20Language%20Models/#model-performance","title":"Model Performance","text":"<p>Foundations of NLP Explained \u2014 Bleu Score and WER Metrics | by Ketan Doshi | Towards Data Science</p>"},{"location":"Artificial%20Intelligence/Generative%20AI%20with%20Large%20Language%20Models/#foundation-models-pricing-","title":"Foundation Models Pricing -","text":"<ul> <li>OpenAI - Foundational models pricing - Pricing (openai.com)<ul> <li>Tokens , Input/Output, Context Window</li> </ul> </li> <li>"},{"location":"Artificial%20Intelligence/Generative%20AI%20with%20Large%20Language%20Models/#reinforcement-learning-with-human-feedback","title":"Reinforcement Learning with Human Feedback","text":"<p>Learning from human preferences (openai.com) - Using RL to provide human cues to the agent to learn the behaviour and quickly take feedback from the humans to optimize its goal function. - This has perils to it as well where the agent tries to trick the human by modofying its policies.</p>"},{"location":"Artificial%20Intelligence/Generative%20AI%20with%20Large%20Language%20Models/#generative-ai-project-lifecycle","title":"Generative AI Project Lifecycle","text":"<p>The Generative AI Life-cycle. The common AI/ML Lifecycle consists of\u2026 | by Ali Arsanjani | Medium ![[Pasted image 20230714052407.png]]</p>"},{"location":"Artificial%20Intelligence/Generative%20AI%20with%20Large%20Language%20Models/#challenges-risks-and-limitations","title":"Challenges, Risks, and Limitations","text":"<ul> <li>Uable to perform complex mathematical computations</li> <li>provide inaccurate information - Hallucination</li> <li>"},{"location":"Artificial%20Intelligence/Generative%20AI%20with%20Large%20Language%20Models/#transformers","title":"Transformers","text":""},{"location":"Artificial%20Intelligence/Generative%20AI%20with%20Large%20Language%20Models/#-the-transformers-architecture-was-proposed-in-a-paper-published-in-2017-by-google-attention-is-all-you-need-170603762pdf-arxivorg","title":"- The transformers' architecture was proposed in a paper published in 2017 by google 'attention is all you need'  1706.03762.pdf (arxiv.org)","text":"<ul> <li>Prior to this existing models such as RNN, LSTM, CNN has the problem of the context understanding if the word was not close.</li> <li>Multi-headed self attention \u2013 12-100 self attention heads. each head is initialized randomly with different weighs. Each head learns different about the word. you cannot control which head focuses on what aspect of the language.</li> <li>Token embedding and position embeddings is used in the transformers that derives its implementation from the word embeddings that were implemented in the word2Vec algorithms.</li> <li>Transformers generally uses vectors of size 512 </li> <li>Simplified transformer architecuture.</li> </ul> <ul> <li> <p>there are various combinations that are possible around this architecture -  ![[Pasted image 20230712224342.png]]</p> </li> <li> <p>Encoder only models - BERT - used for classification tasks. input and output of the same length. The use is less common these days. add additionaly layers to the transformer - sentiment analysis.</p> </li> <li>Encoder-Decoder -- sequeunce to sequence input of a given length and output of variable lenght  - T5, general text generation, BART, and T5. translation.</li> <li>Decoder only - most common- these are generalized to most tasks. GPT , BLOOM, Jurassic, LAMA ..</li> <li>The open </li> </ul>"},{"location":"Artificial%20Intelligence/Generative%20AI%20with%20Large%20Language%20Models/#generative-ai-modalities","title":"Generative AI Modalities","text":"<ul> <li>Chats - All the text associated tasks are associated with next word prediction concept.<ul> <li>Translation</li> <li>Text summarization</li> <li>perform actions such as - meeting minutes, write email</li> <li>Language to code</li> <li>Entity extraction - smaller focussed tasks - kind of word classification</li> <li>Augmenting LLM with external APIs that are invoked by LLM for real time data fetch from other databases.</li> <li> </li> <li>Text to image</li> <li>Text to code</li> </ul> <p>![[Pasted image 20230714051056.png]]</p>"},{"location":"Artificial%20Intelligence/Generative%20AI%20with%20Large%20Language%20Models/#references-","title":"References -","text":"<ul> <li>Coursera Course): Generative AI with Large Language Models by Deeplearning.ai in partnership with AWS</li> <li> </li> <li> <p>Attention is All You Need - This paper introduced the Transformer architecture, with the core \u201cself-attention\u201d mechanism. This article was the foundation for LLMs.</p> </li> <li> <p>BLOOM: BigScience 176B Model - BLOOM is a open-source LLM with 176B parameters (similar to GPT-4) trained in an open and transparent way. In this paper, the authors present a detailed discussion of the dataset and process used to train the model. You can also see a high-level overview of the model here.</p> </li> <li> <p>Vector Space Models - Series of lessons from DeepLearning.AI's Natural Language Processing specialization discussing the basics of vector space models and their use in language modeling.</p> </li> </ul>"},{"location":"Artificial%20Intelligence/Generative%20AI%20with%20Large%20Language%20Models/#transformer-architecture","title":"Transformer Architecture","text":""},{"location":"Artificial%20Intelligence/Generative%20AI%20with%20Large%20Language%20Models/#pre-training-and-scaling-laws","title":"Pre-training and scaling laws","text":"<ul> <li>Scaling Laws for Neural Language Models\u00a0- empirical study by researchers at OpenAI exploring the scaling laws for large language models.</li> </ul>"},{"location":"Artificial%20Intelligence/Generative%20AI%20with%20Large%20Language%20Models/#model-architectures-and-pre-training-objectives","title":"Model architectures and pre-training objectives","text":"<ul> <li> <p>What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization? - The paper examines modeling choices in large pre-trained language models and identifies the optimal approach for zero-shot generalization.</p> </li> <li> <p>HuggingFace Tasks and Model Hub - Collection of resources to tackle varying machine learning tasks using the HuggingFace library.</p> </li> <li> <p>LLaMA: Open and Efficient Foundation Language Models - Article from Meta AI proposing Efficient LLMs (their model with 13B parameters outperform GPT3 with 175B parameters on most benchmarks)</p> </li> </ul>"},{"location":"Artificial%20Intelligence/Generative%20AI%20with%20Large%20Language%20Models/#scaling-laws-and-compute-optimal-models","title":"Scaling laws and compute-optimal models","text":"<ul> <li> <p>Language Models are Few-Shot Learners - This paper investigates the potential of few-shot learning in Large Language Models.</p> </li> <li> <p>Training Compute-Optimal Large Language Models - Study from DeepMind to evaluate the optimal model size and number of tokens for training LLMs. Also known as \u201cChinchilla Paper\u201d.</p> </li> <li> <p>BloombergGPT: A Large Language Model for Finance - LLM trained specifically for the finance domain, a good example that tried to follow chinchilla laws.</p> </li> </ul>"},{"location":"Artificial%20Intelligence/Generative%20AI%20with%20Large%20Language%20Models/#multi-task-instruction-fine-tuning","title":"Multi-task, instruction fine-tuning","text":"<ul> <li> <p>Scaling Instruction-Finetuned Language Models - Scaling fine-tuning with a focus on task, model size and chain-of-thought data.</p> </li> <li> <p>Introducing FLAN: More generalizable Language Models with Instruction Fine-Tuning - This blog (and article) explores instruction fine-tuning, which aims to make language models better at performing NLP tasks with zero-shot inference.</p> </li> </ul>"},{"location":"Artificial%20Intelligence/Generative%20AI%20with%20Large%20Language%20Models/#model-evaluation-metrics","title":"Model Evaluation Metrics","text":"<ul> <li> <p>HELM - Holistic Evaluation of Language Models - HELM is a living benchmark to evaluate Language Models more transparently.</p> </li> <li> <p>General Language Understanding Evaluation (GLUE) benchmark\u00a0- This paper introduces GLUE, a benchmark for evaluating models on diverse natural language understanding (NLU) tasks and emphasizing the importance of improved general NLU systems.</p> </li> <li> <p>SuperGLUE - This paper introduces SuperGLUE, a benchmark designed to evaluate the performance of various NLP models on a range of challenging language understanding tasks.</p> </li> <li> <p>ROUGE: A Package for Automatic Evaluation of Summaries - This paper introduces and evaluates four different measures (ROUGE-N, ROUGE-L, ROUGE-W, and ROUGE-S) in the ROUGE summarization evaluation package, which assess the quality of summaries by comparing them to ideal human-generated summaries.</p> </li> <li> <p>Measuring Massive Multitask Language Understanding (MMLU) - This paper presents a new test to measure multitask accuracy in text models, highlighting the need for substantial improvements in achieving expert-level accuracy and addressing lopsided performance and low accuracy on socially important subjects.</p> </li> <li> <p>BigBench-Hard - Beyond the Imitation Game: Quantifying and Extrapolating the Capabilities of Language Models - The paper introduces BIG-bench, a benchmark for evaluating language models on challenging tasks, providing insights on scale, calibration, and social bias.</p> </li> </ul>"},{"location":"Artificial%20Intelligence/Generative%20AI%20with%20Large%20Language%20Models/#parameter-efficient-fine-tuning-peft","title":"Parameter- efficient fine tuning (PEFT)","text":"<ul> <li> <p>Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning - This paper provides a systematic overview of Parameter-Efficient Fine-tuning (PEFT) Methods in all three categories discussed in the lecture videos.</p> </li> <li> <p>On the Effectiveness of Parameter-Efficient Fine-Tuning - The paper analyzes sparse fine-tuning methods for pre-trained models in NLP.</p> </li> </ul>"},{"location":"Artificial%20Intelligence/Generative%20AI%20with%20Large%20Language%20Models/#lora","title":"LoRA","text":"<ul> <li> <p>LoRA Low-Rank Adaptation of Large Language Models - This paper proposes a parameter-efficient fine-tuning method that makes use of low-rank decomposition matrices to reduce the number of trainable parameters needed for fine-tuning language models.</p> </li> <li> <p>QLoRA: Efficient Finetuning of Quantized LLMs - This paper introduces an efficient method for fine-tuning large language models on a single GPU, based on quantization, achieving impressive results on benchmark tests.</p> </li> </ul>"},{"location":"Artificial%20Intelligence/Generative%20AI%20with%20Large%20Language%20Models/#prompt-tuning-with-soft-prompts","title":"Prompt tuning with soft prompts","text":"<ul> <li>The Power of Scale for Parameter-Efficient Prompt Tuning - The paper explores \"prompt tuning,\" a method for conditioning language models with learned soft prompts, achieving competitive performance compared to full fine-tuning and enabling model reuse for many tasks.</li> </ul>"},{"location":"Artificial%20Intelligence/Job%20Description%20of%20a%20Prompt%20Engineer/","title":"Job Description of a Prompt Engineer","text":""},{"location":"Artificial%20Intelligence/Job%20Description%20of%20a%20Prompt%20Engineer/#prompt-engineering","title":"Prompt engineering","text":"<ul> <li>A Typical job description generated using ChatGPT for prompt engineering</li> </ul> <p>Role Overview: We are seeking a skilled and innovative Prompt Engineer to join our team. As a Prompt Engineer, you will play a crucial role in developing and refining the prompt engineering process for our language model systems. Your expertise in natural language processing (NLP) and deep learning will be instrumental in creating high-quality prompts that effectively guide the model's output. You will collaborate closely with our research and development teams to enhance the performance and capabilities of our language models.</p> <p>Responsibilities: 1. Develop and refine the prompt engineering process to optimize model performance and ensure the generation of coherent and contextually relevant outputs. 2. Collaborate with research and development teams to understand the requirements and goals of specific projects and tasks. 3. Design and implement prompt engineering strategies to tailor the behavior and responses of the language models. 4. Conduct comprehensive analyses of prompt design and its impact on model performance, providing insights and recommendations for improvement. 5. Explore and experiment with various prompt engineering techniques, including conditioning, token manipulation, and prompt format optimization. 6. Continuously monitor and evaluate model outputs to identify areas for improvement and implement necessary prompt adjustments. 7. Collaborate with cross-functional teams to address prompt-related challenges and provide prompt engineering solutions. 8. Stay updated with the latest advancements in NLP and prompt engineering techniques, integrating relevant research findings into the development process. 9. Assist in the development of tools and frameworks to streamline the prompt engineering workflow and ensure efficiency and scalability. 10. Contribute to research papers, documentation, and knowledge sharing activities to showcase the impact of prompt engineering on model performance.</p> <p>Requirements: 1. Bachelor's or Master's degree in Computer Science, Electrical Engineering, or a related field. A Ph.D. is a plus. 2. Strong understanding of natural language processing, deep learning, and machine learning techniques. 3. Proven experience in prompt engineering, language model development, or related NLP tasks. 4. Proficiency in programming languages such as Python, along with experience using relevant libraries and frameworks (e.g., TensorFlow, PyTorch). 5. Solid understanding of NLP concepts, including attention mechanisms, sequence modeling, and neural network architectures. 6. Familiarity with prompt engineering strategies and techniques, such as conditioning, control codes, and input manipulation. 7. Ability to analyze and interpret model outputs to identify areas for improvement and implement effective prompt adjustments. 8. Excellent problem-solving skills and the ability to think critically and creatively in a fast-paced research environment. 9. Strong collaboration and communication skills, with the ability to work effectively in cross-functional teams. 10. Proactive and self-motivated attitude, with a keen interest in staying updated with the latest advancements in NLP and prompt engineering.</p> <p>Join our team as a Prompt Engineer and contribute to the advancement of state-of-the-art language models. Help shape the behavior and capabilities of our models by designing and implementing effective prompts that drive superior performance and enhance user experiences.</p>"},{"location":"CICD/Chapter-Containers%20and%20Kubernetes/About%20Docker/","title":"Docker Notes","text":""},{"location":"CICD/Chapter-Containers%20and%20Kubernetes/About%20Docker/#docker-notes","title":"Docker Notes","text":"<p>These notes are based on the LinkedIn course -  Docker for developers by Emmanuel Henri - Docker for developers (linkedin.com)</p>"},{"location":"CICD/Chapter-Containers%20and%20Kubernetes/About%20Docker/#need-for-docker","title":"Need for Docker","text":"<ul> <li>Docker is needed in the scenarios where you need to setup new projects with the same settings and configuration across different servers, desktops.</li> <li>Docker provides container where all the project specific settings, its inversion of control and the resources required for the code to run are packaged within the container.</li> <li>A Container is the location where all the resources required by the application exists. </li> <li>The container itself is immutable and is static therefore it requires volume to store the data. It is external disk to which the data is stored. This is required in case of running database as containers that can then store the data on external volumes.</li> <li>In addition, container by itself has no idea about the external environment and the networking. therefore for the container to talk to external environment you will need to configure the networking to enable it to talk to one-another.</li> </ul>"},{"location":"CICD/Chapter-Containers%20and%20Kubernetes/About%20Docker/#core-components-of-container-based-development","title":"Core Components of Container based Development","text":""},{"location":"CICD/Chapter-Containers%20and%20Kubernetes/About%20Docker/#tools-utilized-in-docker-based-development","title":"Tools Utilized in Docker based Development","text":"<pre><code>flowchart LR\n\nid1[Visual Studio Code]\nid2[Docker Desktop]\nid3[Kubernetes/Kubectl]\nid4[Minikube]\nid5[Docker Compose]\nid6[Docker Swarm]\nid7[Github]\nid8[Travis CI]\nid9[Docker hub]\nid10[GCP or AWS Kubernetes Cluster]\n\nDeveloper -- Coding --&gt; id1\nid1 -- Create Image / build --&gt; id2\nid1 -- Code commit --&gt; id7\nid7 -- CI pipeline --&gt; id8\nid8 -- push image --&gt; id9\nid3 --&gt; id4\nid9 -- pull image from repo --&gt;id4\nid5 -- stack deployment --&gt; id6\nid1 --&gt; id5\nid9 -- production deployment --&gt; id10\n</code></pre>"},{"location":"CICD/Chapter-Containers%20and%20Kubernetes/About%20Docker/#docker-desktop-installation-on-windows","title":"Docker Desktop Installation on Windows","text":"<ul> <li>When installing docker desktop, download the docker for windows.</li> <li>This might require installation of an older version of WSL to run it on the desktop. This is done from Microsoft website.<ul> <li>WSL stands for windows subsystem for Linux. Using this you are able to run a Linux environment in windows.</li> <li>Manual installation steps for older versions of WSL | Microsoft Docs</li> </ul> </li> </ul> <p>The commands for docker are available at Docker-Commands</p> <p>Once docker desktop is installed a simple docker based POC conducted can be performed following steps mentioned in Simple Docker Project</p> <ul> <li>These notes are based on my understanding of the topic. Some of the content and images refers to the online articles, learning courses I attended. I encourage you to lookup the links provided to develop your own understanding of it at deeper level.</li> <li>The intention of these notes is to serve as reference marker to other sites, online courses, books that I found useful while learning about it.</li> <li>If you would like to share any thoughts or give comments please feel free to reach me @ abhishekgupta86@gmail.com or @ www.linkedin.com/in/abhishekgupta86</li> </ul>"},{"location":"CICD/Chapter-Containers%20and%20Kubernetes/CI%20with%20Travis/","title":"Travis based Continuous Integration","text":""},{"location":"CICD/Chapter-Containers%20and%20Kubernetes/CI%20with%20Travis/#travis-based-continuous-integration","title":"Travis based Continuous Integration","text":"<p>These notes are based on the LinkedIn course -  Docker for developers by Emmanuel Henri - Docker for developers (linkedin.com)</p> <ul> <li>Travis CI is a cloud based managed service that can be used for integrating it with a GitHub repos to continuously build and compile the repo.</li> <li>Similar to Travis there are many other CI platforms that can be used to achieve the same objective. example - Circle CI, Azure CI etc.</li> <li>Travis is a ruby based CI platform that gets triggered based on the .travis.yml file that must be present at the root of the project folder that you need to build<ul> <li>To check for the file that has been used for this POC refer the code repository at Github - abhishekgupta-myrepo/travis-ci_poc (github.com)</li> </ul> </li> <li>Travis platform gets trigged based on everytime a commit happens to the repository that has been configured with Travis for build.</li> <li>In order to use Travis you will need to create an account with Travis and will have to specify the repo in github that you would like to be configured.</li> </ul>"},{"location":"CICD/Chapter-Containers%20and%20Kubernetes/CI%20with%20Travis/#process-for-continuous-integration","title":"Process for Continuous Integration","text":"<pre><code>flowchart\u00a0LR  \n\u00a0\u00a0\u00a0\u00a0id1([develop new function])\n\u00a0\u00a0\u00a0\u00a0id2([commit changes to repo])\n    id3([CI provider tests the build])\n    id4([deploy the code])\n    id1--&gt;id2\n    id2--&gt; id3\n    id3--&gt; id4\n    id4--&gt; id1\n</code></pre> - Docker fits well into this process where each time the build is triggered in Tavis it can compile and then create the docker image that can be pushed onto a remote repo. Using Docker in Builds - Travis CI (travis-ci.com) <ul> <li> <p>You can view the history of the build as well as view the current build that is in process. Travis provides you with console where you can view the logs as the build is in progress. </p> </li> <li> <p>The same .travis.yml can the be used to push the image that has been created into the remote repo such as docker hub. </p> </li> <li> <p>At the end of this CI pipeline a CD (Continuous Deployment) platform can then take up the image pushed into the remote repo to update the Kubernetes cluster based environment running.</p> </li> </ul> <ul> <li>These notes are based on my understanding of the topic. Some of the content and images refers to the online articles, learning courses I attended. I encourage you to lookup the links provided to develop your own understanding of it at deeper level.</li> <li>The intention of these notes is to serve as reference marker to other sites, online courses, books that I found useful while learning about it.</li> <li>If you would like to share any thoughts or give comments please feel free to reach me @ abhishekgupta86@gmail.com or @ www.linkedin.com/in/abhishekgupta86</li> </ul>"},{"location":"CICD/Chapter-Containers%20and%20Kubernetes/Docker%20Compose/","title":"Docker Compose","text":""},{"location":"CICD/Chapter-Containers%20and%20Kubernetes/Docker%20Compose/#docker-compose","title":"Docker Compose","text":"<p>Instructions below are based on following the directions provided during the course :  Docker for developers by Emmanuel Henri - Docker for developers (linkedin.com)</p>"},{"location":"CICD/Chapter-Containers%20and%20Kubernetes/Docker%20Compose/#step-by-step-guide-to-run-docker-compose","title":"Step by Step Guide to Run Docker Compose","text":"<ul> <li>Follow the steps below to develop a full stack service comprising of multiple containers.</li> <li>The instructions for developing a full stack are written the docker compose .yml file. Look at the sample project developed available at the GitHub  - abhishekgupta-myrepo/docker_poc (github.com)</li> <li>Once the docker compose file is written preform the following steps to create a service.</li> <li> <p>Step 1 : Run the below command to create images for all the services that form part of the full stack and must be deployed as a single unit. </p><pre><code>PS C:\\Users\\abhis\\Projects\\docker_poc&gt; docker-compose build\n[+] Building 22.1s (11/11) FINISHED\n =&gt; [internal] load build definition from Dockerfile                                                      0.0s\n =&gt; =&gt; transferring dockerfile: 155B                                                                      0.0s\n =&gt; [internal] load .dockerignore                                                                         0.0s\n =&gt; =&gt; transferring context: 66B                                                                          0.0s\n =&gt; [internal] load metadata for docker.io/library/node:latest                                            1.7s\n =&gt; [auth] library/node:pull token for registry-1.docker.io                                               0.0s\n =&gt; [1/5] FROM docker.io/library/node@sha256:8a45c95c328809e7e10e8c9ed5bf8374620d62e52de1df7ef8e71a9596e  0.0s\n =&gt; [internal] load build context                                                                         0.3s\n =&gt; =&gt; transferring context: 16.08MB                                                                      0.3s\n =&gt; CACHED [2/5] WORKDIR /usr/src/app                                                                     0.0s\n =&gt; [3/5] COPY package*.json ./                                                                           0.1s\n =&gt; [4/5] RUN npm install                                                                                18.3s\n =&gt; [5/5] COPY . .                                                                                        0.1s\n =&gt; exporting to image                                                                                    1.4s\n =&gt; =&gt; exporting layers                                                                                   1.4s\n =&gt; =&gt; writing image sha256:3b656d9f2fa551b00589d89b61f563f07c33c7f8eb2a2ce191a7b6d2d6c14576              0.0s\n =&gt; =&gt; naming to docker.io/library/docker_poc-app                                                         0.0s\n\nUse 'docker scan' to run Snyk tests against images to find vulnerabilities and learn how to fix them\n</code></pre> </li> <li> <p>Step 2 : Run the following command to spin up specific container to be created as part of the full stack. </p> <ul> <li>Note: Even if you bring up each container one at a time even then all the containers form part of the stack. </li> <li>Make sure to trigger the containers in the order so that the other container does not fail that is dependent upon the other container for successful boot. This is done by mentioning the specific container to kickoff</li> <li>Reason for specifying the -d and mentioning the container that needs to start first is that docker compose will start all the containers in parallel and will run into issues if the database is not up. <pre><code>PS C:\\Users\\abhis\\Projects\\docker_poc&gt; docker-compose up -d mongo\n[+] Running 10/10\n - mongo Pulled                                                                                          19.8s\n   - 675920708c8b Pull complete                                                                           5.8s\n   - 6f9c8c301e0f Pull complete                                                                           5.9s\n   - 73738965c4ce Pull complete                                                                           6.3s\n   - 7fd6635b9ddf Pull complete                                                                           6.9s\n   - 73a471eaa4ad Pull complete                                                                           7.0s\n   - bcf274af89b0 Pull complete                                                                           7.1s\n   - 04fc489f2a3b Pull complete                                                                           7.2s\n   - 6eff8a505292 Pull complete                                                                          18.6s\n   - a5ef4431fce7 Pull complete                                                                          18.6s\n[+] Running 2/2\n - Network docker_poc_default  Created                                                                    0.0s\n - Container mongo             Started                                                                    1.1s\nPS C:\\Users\\abhis\\Projects\\docker_poc&gt; \n</code></pre></li> </ul> </li> <li> <p>Step 3 : Run the below command to look into the logs of specific container.</p> <ul> <li>This is not a core part of the sequence of instructions to spin up this stack and is just to validate the state of the containers.  <pre><code>PS C:\\Users\\abhis\\Projects\\docker_poc&gt; docker logs e74cd\n{\"t\":{\"$date\":\"2022-09-15T20:59:49.287+00:00\"},\"s\":\"I\",  \"c\":\"NETWORK\",  \"id\":4915701, \"ctx\":\"-\",\"msg\":\"Initialized wire specification\",\"attr\":{\"spec\":{\"incomingExternalClient\":{\"minWireVersion\":0,\"maxWireVersion\":17},\"incomingInternalClient\":{\"minWireVersion\":0,\"maxWireVersion\":17},\"outgoing\":{\"minWireVersion\":6,\"maxWireVersion\":17},\"isInternalClient\":true}}}\n</code></pre></li> </ul> </li> <li> <p>Step 4 : Run the below command to stop all the containers that form part of a stack built through the docker compose.</p> <ul> <li>Stopping all the containers can be done by a single command</li> <li>Make sure to stop the docker compose stop to stop the service else if you stop the containers individually they will spin up again. <pre><code>PS C:\\Users\\abhis\\Projects\\docker_poc&gt; docker-compose stop\n[+] Running 2/2\n - Container app    Stopped                                                                               1.0s \n - Container mongo  Stopped \n</code></pre></li> </ul> </li> <li> <p>Once the docker compose is ready for deployment it can be used for deployment through container orchestration tools such as - </p> <ul> <li>Docker Swarm: look at the notes @ Docker-Swarm. This is native to Docker and comes installed with docker desktop.</li> <li>Kubernetes: look at the notes @Kubernetes. It is the current industry standard. Instead of setting up your own Kubernetes cluster most common practice is to use it as a managed services through cloud providers such as GCP or AWS.</li> <li>Mesos</li> </ul> </li> </ul> <ul> <li>These notes are based on my understanding of the topic. Some of the content and images refers to the online articles, learning courses I attended. I encourage you to lookup the links provided to develop your own understanding of it at deeper level.</li> <li>The intention of these notes is to serve as reference marker to other sites, online courses, books that I found useful while learning about it.</li> <li>If you would like to share any thoughts or give comments please feel free to reach me @ abhishekgupta86@gmail.com or @ www.linkedin.com/in/abhishekgupta86</li> </ul>"},{"location":"CICD/Chapter-Containers%20and%20Kubernetes/Docker-Commands/","title":"Docker Commands Cheat sheet","text":""},{"location":"CICD/Chapter-Containers%20and%20Kubernetes/Docker-Commands/#docker-commands-cheat-sheet","title":"Docker Commands Cheat sheet","text":""},{"location":"CICD/Chapter-Containers%20and%20Kubernetes/Docker-Commands/#get-docker-options-on-bash","title":"Get Docker Options on Bash","text":"<ul> <li>Command below provides all the options available with Docker. All the docker commands start wit the word docker <pre><code>AGupta@vm2 MINGW64 ~ $ docker\nUsage: docker [OPTIONS] COMMAND\nOptions:\n--config string Location of client config files (default \"C:UsersAGupta.docker\")\n-c, --context string Name of the context to use to connect to the daemon (overrides DOCKER_HOST env var and default context set with \"docker context use\")\n-D, --debug Enable debug mode\n-H, --host list Daemon socket(s) to connect to\n-l, --log-level string Set the logging level (\"debug\"|\"info\"|\"warn\"|\"error\"|\"fatal\") (default \"info\")\n--tls Use TLS; implied by --tlsverify\n--tlscacert string Trust certs signed only by this CA (default \"C:UsersAGupta.dockerca.pem\")\n--tlscert string Path to TLS certificate file (default \"C:UsersAGupta.dockercert.pem\")\n--tlskey string Path to TLS key file (default \"C:UsersAGupta.dockerkey.pem\")\n--tlsverify Use TLS and verify the remote\n-v, --version Print version information and quit\n</code></pre></li> </ul>"},{"location":"CICD/Chapter-Containers%20and%20Kubernetes/Docker-Commands/#management-commands","title":"Management Commands","text":"Command Description builder Manage builds config Manage Docker configs container Manage containers context Manage contexts image Manage images network Manage networks node Manage Swarm nodes plugin Manage plugins secret Manage Docker secrets service Manage services stack Manage Docker stacks swarm Manage Swarm system Manage Docker trust Manage trust on Docker images volume Manage volumes"},{"location":"CICD/Chapter-Containers%20and%20Kubernetes/Docker-Commands/#docker-commands","title":"Docker Commands","text":"Command Description attach Attach local standard input, output, and error streams to a running container build Build an image from a Dockerfile commit Create a new image from a container's changes cp Copy files/folders between a container and the local filesystem create Create a new container deploy Deploy a new stack or update an existing stack diff Inspect changes to files or directories on a container's filesystem events Get real time events from the server exec Run a command in a running container export Export a container's filesystem as a tar archive history Show the history of an image images List images import Import the contents from a tarball to create a filesystem image info Display system-wide information inspect Return low-level information on Docker objects kill Kill one or more running containers load Load an image from a tar archive or STDIN login Log in to a Docker registry logout Log out from a Docker registry logs Fetch the logs of a container pause Pause all processes within one or more containers port List port mappings or a specific mapping for the container ps List containers pull Pull an image or a repository from a registry push Push an image or a repository to a registry rename Rename a container restart Restart one or more containers rm Remove one or more containers rmi Remove one or more images run Run a command in a new container save Save one or more images to a tar archive (streamed to STDOUT by default) search Search the Docker Hub for images start Start one or more stopped containers stats Display a live stream of container(s) resource usage statistics stop Stop one or more running containers tag Create a tag TARGET_IMAGE that refers to SOURCE_IMAGE top Display the running processes of a container unpause Unpause all processes within one or more containers update Update configuration of one or more containers version Show the Docker version information wait Block until one or more containers stop, then print their exit codes <ul> <li>Any of the commands described above can be used by following the syntax below -  <pre><code>Run 'docker COMMAND --help' for more information on a command.\n</code></pre></li> <li>To understand how to run a simple Docker project look at Simple Docker Project</li> </ul>"},{"location":"CICD/Chapter-Containers%20and%20Kubernetes/Docker-Swarm/","title":"Introduction to Swarm","text":""},{"location":"CICD/Chapter-Containers%20and%20Kubernetes/Docker-Swarm/#introduction-to-swarm","title":"Introduction to Swarm","text":"<p>Instructions below are based on following the directions provided during the course :  Docker for developers by Emmanuel Henri - Docker for developers (linkedin.com)</p>"},{"location":"CICD/Chapter-Containers%20and%20Kubernetes/Docker-Swarm/#docker-swarm-overview","title":"Docker Swarm Overview","text":"<ul> <li>Docker Swarm is a tool to manage cluster of nodes. Each node runs multiple containers. A node is a physical or a virtual machine on which multiple containers are run.</li> <li>The swarm then provides the ability to load balance the nodes and manage multiple nodes within the clusters.</li> </ul> <pre><code>flowchart\u00a0TB  \n\u00a0\u00a0\n\u00a0\u00a0\u00a0\u00a0subgraph\u00a0Node one  \n\u00a0\u00a0\u00a0\u00a0a1--&gt;a2  \n\u00a0\u00a0\u00a0\u00a0end  \n\u00a0\u00a0\u00a0\u00a0subgraph\u00a0Node two  \n\u00a0\u00a0\u00a0\u00a0b1--&gt;b2  \n\u00a0\u00a0\u00a0\u00a0end  \n\u00a0\u00a0\u00a0\u00a0subgraph\u00a0Node three  \n\u00a0\u00a0\u00a0\u00a0c1--&gt;c2  \n\u00a0\u00a0\u00a0\u00a0end  \n\u00a0\u00a0\u00a0\n</code></pre> - Docker swarm works on the principle of master and slave model. The master node has the swarm that manages the worked nodes on which the containers are deployed. - On a local machine you can setup a single node swarm where the same node can be used for running the containers and the swarm to manage it. - On the servers each node that needs to be joined with the master must be joined and must have the docker installed. Once it is joined to the master node Swarm it then takes care of deploying the containers on the nodes. - Docker swarm comes installed with docker desktop."},{"location":"CICD/Chapter-Containers%20and%20Kubernetes/Docker-Swarm/#steps-to-run-docker-swarm-cluster-locally","title":"Steps to Run Docker Swarm Cluster Locally","text":"<p>Follow the below commands to run the docker swarm locally. This requires the == docker compose .yml== file that you can refer from the GitHub project @  - abhishekgupta-myrepo/docker_poc (github.com) -  Step 1 : Run the below command in PowerShell from Visual Studio code  </p><pre><code>PS C:\\Users\\abhis\\Projects\\docker_poc&gt; docker swarm init\nSwarm initialized: current node (82w06t5v7kdki2pncbb93ivvy) is now a manager.\n\nTo add a worker to this swarm, run the following command:\n\n    docker swarm join --token SWMTKN-1-4mou11qdvv4zggmgdhrtq33kmphvhrxtqe0pqq99ow9foxbyp5-e7868tp81uaun9qtkcrel1n6h 192.168.65.3:2377\n\nTo add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.\n</code></pre> <ul> <li>You need to ssh into the machine that you want to add to the swarm where you need to run the command <pre><code>docker swarm join --token SWMTKN-1-4mou11qdvv4zggmgdhrtq33kmphvhrxtqe0pqq99ow9foxbyp5-e7868tp81uaun9qtkcrel1n6h 192.168.65.3:2377\n</code></pre></li> <li>Step 2 : Check for the  information about the docker setup that will also provide details about the docker swam that has been initialized. <pre><code>PS C:\\Users\\abhis\\Projects\\docker_poc&gt; docker info\nClient:\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc., v0.9.1)\n  compose: Docker Compose (Docker Inc., v2.10.2)\n  extension: Manages Docker extensions (Docker Inc., v0.2.9)\n  sbom: View the packaged-based Software Bill Of Materials (SBOM) for an image (Anchore Inc., 0.6.0)\n  scan: Docker Scan (Docker Inc., v0.19.0)\n\nServer:\n Containers: 0\n  Running: 0\n  Paused: 0\n  Stopped: 0\n Images: 0\n Server Version: 20.10.17\n Storage Driver: overlay2\n  Backing Filesystem: extfs\n  Supports d_type: true\n  Native Overlay Diff: true\n  userxattr: false\n Logging Driver: json-file\n Cgroup Driver: cgroupfs\n Cgroup Version: 1\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\n Swarm: active\n  NodeID: 82w06t5v7kdki2pncbb93ivvy\n  Is Manager: true\n  ClusterID: wd8zdd14dyeflloa4jzdpdaoo\n  Managers: 1\n  Nodes: 1\n  Default Address Pool: 10.0.0.0/8\n  SubnetSize: 24\n  Data Path Port: 4789\n  Orchestration:\n   Task History Retention Limit: 5\n  Raft:\n   Snapshot Interval: 10000\n   Number of Old Snapshots to Retain: 0\n   Heartbeat Tick: 1\n   Election Tick: 10\n  Dispatcher:\n   Heartbeat Period: 5 seconds\n  CA Configuration:\n   Expiry Duration: 3 months\n   Force Rotate: 0\n  Autolock Managers: false\n  Root Rotation In Progress: false\n  Node Address: 192.168.65.3\n  Manager Addresses:\n   192.168.65.3:2377\n Runtimes: io.containerd.runc.v2 io.containerd.runtime.v1.linux runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: 9cd3357b7fd7218e4aec3eae239db1f68a5a6ec6\n runc version: v1.1.4-0-g5fd4c4d\n init version: de40ad0\n Security Options:\n  seccomp\n   Profile: default\n No Proxy: hubproxy.docker.internal\n Registry: https://index.docker.io/v1/\n Labels:\n Experimental: false\n Insecure Registries:\n  hubproxy.docker.internal:5000\n  127.0.0.0/8\n Live Restore Enabled: false\n</code></pre></li> <li>Step 3 : Look at the node that has been created -  <pre><code>PS C:\\Users\\abhis\\Projects\\docker_poc&gt; docker node ls\nID                            HOSTNAME         STATUS    AVAILABILITY   MANAGER STATUS   ENGINE VERSION\n82w06t5v7kdki2pncbb93ivvy *   docker-desktop   Ready     Active         Leader           20.10.17\nPS C:\\Users\\abhis\\Projects\\docker_poc&gt; \n</code></pre></li> <li>Step 4 : Run the below command to deploy the stack on docker swarm.<ul> <li>For this command to run make sure that the image name is mentioned in the docker-compose file and the version of the docker-compose.yml is 3.0 <pre><code>PS C:\\Users\\abhis\\Projects\\docker_poc&gt; docker stack deploy -c docker-compose.yml STACK\nIgnoring unsupported options: build, links, restart\n\nIgnoring deprecated options:\n\ncontainer_name: Setting the container name is not supported.\n\nexpose: Exposing ports is unnecessary - services on the same network can access each other's containers on any port.\n\nUpdating service STACK_client (id: jaojw6zb20dihl7wgwkweu79c)\nimage clientapp:latest could not be accessed on a registry to record\nits digest. Each node will access clientapp:latest independently,\npossibly leading to different nodes running different\nversions of the image.\n\nCreating service STACK_app\nCreating service STACK_mongo\nPS C:\\Users\\abhis\\Projects\\docker_poc&gt; \n</code></pre></li> </ul> </li> <li>Step 5 : Validate that the docker containers are up and running. <ul> <li>If you check all the containers will be instantiated as part of the stack that you described in the docker compose .yml and will be prefixed with the name that you provided to the stack at the time of docker stack deploy command <pre><code>PS C:\\Users\\abhis\\Projects\\docker_poc&gt; docker service ls\nID             NAME           MODE         REPLICAS   IMAGE               PORTS\n303uyn5tfhsg   STACK_app      replicated   1/1        backendapp:latest   *:4000-&gt;4000/tcp\njaojw6zb20di   STACK_client   replicated   1/1        clientapp:latest    *:3000-&gt;3000/tcp\nnujukyytj1jl   STACK_mongo    replicated   1/1        mongo:latest        *:27017-&gt;27017/tcp\nPS C:\\Users\\abhis\\Projects\\docker_poc&gt; \n</code></pre></li> </ul> </li> <li>Step 6 : Stop the docker swarm based services using the command below - <ul> <li>Stopping the stack service is important else it will keep running. <pre><code>-PS C:\\Users\\abhis\\Projects\\docker_poc&gt; docker stack rm STACK\nRemoving service STACK_app\nRemoving service STACK_client\nRemoving service STACK_mongo\nRemoving network STACK_default\n</code></pre></li> </ul> </li> </ul> <p>To develop a CI pipeline from Github project using Travis CI app refer to the notes on CI with Travis     - This takes it to full circle where once the code is developed, tested locally, and committed to the git repo in Github a continuous integration tool such as Travis can be used for continuous build that gets triggered for each commit to compile the code and push the image into a remote repository such as docker hub.</p> <ul> <li>These notes are based on my understanding of the topic. Some of the content and images refers to the online articles, learning courses I attended. I encourage you to lookup the links provided to develop your own understanding of it at deeper level.</li> <li>The intention of these notes is to serve as reference marker to other sites, online courses, books that I found useful while learning about it.</li> <li>If you would like to share any thoughts or give comments please feel free to reach me @ abhishekgupta86@gmail.com or @ www.linkedin.com/in/abhishekgupta86</li> </ul>"},{"location":"CICD/Chapter-Containers%20and%20Kubernetes/Kubernetes/","title":"Simple Kubernetes Project","text":""},{"location":"CICD/Chapter-Containers%20and%20Kubernetes/Kubernetes/#simple-kubernetes-project","title":"Simple Kubernetes Project","text":""},{"location":"CICD/Chapter-Containers%20and%20Kubernetes/Kubernetes/#kubernetes-background","title":"Kubernetes Background","text":"<ul> <li>A Kubernetes cluster can be deployed and tested locally following the steps mentioned below - <ul> <li>This simplicity must not be confused with the actual complexity and the depth of the functionality that Kubernetes provides for running production clusters.</li> <li>Kubernetes is currently far and wide an industry standard. This is generally used for running 10000+ nodes clusters.</li> <li>It is now an open source project originally developed by google.</li> <li>Mesos is the nearest competitor that is used by many giants for running their production environment.</li> </ul> </li> </ul>"},{"location":"CICD/Chapter-Containers%20and%20Kubernetes/Kubernetes/#steps-to-run-kubernetes-cluster-locally","title":"Steps to Run Kubernetes Cluster Locally","text":"<ul> <li> <p>Step 1 : Install Kubectl from the website for the local deployment of the cluster</p> <ul> <li>Getting started | Kubernetes </li> </ul> </li> <li> <p>Step 2 : Install Kind or Minikube which is used for running deployments in local or in development environment. Both the options are provided on the Kubernetes website. minikube start | minikube (k8s.io)</p> <ul> <li>Run the command minikube start . Note this must be run from the command prompt<ul> <li>Ensure that the .exe file that you downloaded for the minikube is set in the PATH environment variable.</li> <li>Ensure that prior to starting the minikube cluster you have installed kubectl <ul> <li>The steps to install kubectl [which the command line for Kubernetes] are similar to installing the minikube. i.e. once the .exe file for kubectl has been downloaded setup the PATH variable for kubectl.exe file.</li> </ul> </li> </ul> </li> </ul> </li> </ul> <p></p> <ul> <li>Step 3 : Run this command  to get information about the cluster kubectl cluster-info</li> <li>Step 4: Run this command to deploy the container image in the minikube cluster. ==kubectl create deployment nodeapplication2 -- image=node ==<ul> <li>Note: Minikube pulls the image provided either from Dockerhub by default or from the specified repo that can be mentioned as the full alternate url to the image.</li> <li>Unlike docker swarm that pulls the images and the services stack from the docker compose .yml minikube pulls images directly that have been pushed into the repo.</li> <li>So in case you need test the local cluster deployment you will need to push the image to Docker hub repo from where it will be pulled.</li> </ul> </li> <li>Step 5 : Run this command to check the deployment kubectl get deployments</li> </ul> <p>To develop a CI pipeline from Github project using Travis CI app refer to the notes on CI with Travis     - This takes it to full circle where once the code is developed, tested locally, and committed to the git repo in Github a continuous integration tool such as Travis can be used for continuous build that gets triggered for each commit to compile the code and push the image into a remote repository such as docker hub.</p>"},{"location":"CICD/Chapter-Containers%20and%20Kubernetes/Simple%20Docker%20Project/","title":"Compile simple project in docker","text":""},{"location":"CICD/Chapter-Containers%20and%20Kubernetes/Simple%20Docker%20Project/#compile-simple-project-in-docker","title":"Compile simple project in docker","text":"<p>Instructions below are based on following the directions provided during the course :  Docker for developers by Emmanuel Henri - Docker for developers (linkedin.com)</p>"},{"location":"CICD/Chapter-Containers%20and%20Kubernetes/Simple%20Docker%20Project/#step-to-create-docker-based-project","title":"Step to Create Docker based Project","text":"<p>Follow the below steps to setup a simple backend container based docker project.</p> <ul> <li>Step 1 - Create the Dockerfile. Refer to the GitHub project - abhishekgupta-myrepo/docker_poc (github.com)<ul> <li>The backend project in within the /app folder in the git project.</li> <li>To run and compile the docker image, go to the app folder in the Visual studio code PowerShell terminal to type in the commands below - </li> <li>Before you run the commands make sure you have the docker desktop installed. Follow the steps mentioned in About Docker</li> <li>Make sure you have installed the VS code extension as it helps in suggesting the commands when writing Dockerfile.</li> <li>All the commands are uppercase</li> </ul> </li> <li>Step 2 - Run the docker command to build the image -  <pre><code>PS C:\\Users\\abhis\\Projects\\docker_poc&gt; docker --version\nDocker version 20.10.17, build 100c701\n\nPS C:\\Users\\abhis\\Projects\\docker_poc&gt; docker build -t simplebackend .\n[+] Building 51.1s (11/11) FINISHED\n =&gt; [internal] load build definition from Dockerfile                                                                      0.1s\n =&gt; =&gt; transferring dockerfile: 155B                                                                                      0.0s\n =&gt; [internal] load .dockerignore                                                                                         0.0s\n =&gt; =&gt; transferring context: 66B                                                                                          0.0s\n =&gt; [internal] load metadata for docker.io/library/node:latest                                                            2.1s\n =&gt; [auth] library/node:pull token for registry-1.docker.io                                                               0.0s\n =&gt; [internal] load build context                                                                                         0.1s\n =&gt; =&gt; transferring context: 30.39kB                                                                                      0.0s \n =&gt; [1/5] FROM docker.io/library/node@sha256:8a45c95c328809e7e10e8c9ed5bf8374620d62e52de1df7ef8e71a9596ec8676            30.5s \n =&gt; =&gt; resolve \n =&gt; =&gt; extracting sha256:8471b75885efc7790a16be5328e3b368567b76a60fc3feabd6869c15e175ee05                                 4.7s \n...... Multiple lines of progress......\n =&gt; =&gt; extracting sha256:b51256989aaaa6444b004b6f1dc3753c7c5a0f2132187622ee395e1327c36061                                 0.0s \n =&gt; [2/5] WORKDIR /usr/src/app                                                                                            0.5s \n =&gt; [3/5] COPY package*.json ./                                                                                           0.0s \n =&gt; [4/5] RUN npm install                                                                                                16.4s \n =&gt; [5/5] COPY . .                                                                                                        0.0s \n =&gt; exporting to image                                                                                                    1.4s \n =&gt; =&gt; exporting layers                                                                                                   1.4s \n =&gt; =&gt; writing image sha256:27a88271fc4f9634fe0a96e8e79ad533303f454f01966daa78fa5562931c4760                              0.0s \n =&gt; =&gt; naming to docker.io/library/simplebackend                                                                          0.0s \n\nUse 'docker scan' to run Snyk tests against images to find vulnerabilities and learn how to fix them\nPS C:\\Users\\abhis\\Projects\\docker_poc&gt;\n</code></pre></li> <li>Step 3 - Run the following docker command to check the image built locally-  <pre><code>PS C:\\Users\\abhis\\Projects\\docker_poc&gt; docker images\nREPOSITORY      TAG       IMAGE ID       CREATED         SIZE\nsimplebackend   latest    27a88271fc4f   6 minutes ago   1.05GB\nalpine/git      latest    692618a0d74d   2 weeks ago     43.4MB\n</code></pre></li> <li>Step 4 - Run the following docker command to instantiate container based on the image that you just created in the previous step-<ul> <li>Mention about the port using -p and bind the container port to the local machine/VM port as show in the command below.</li> <li>Refer to the image using the tag that you assigned during the build process. <pre><code>PS C:\\Users\\abhis\\Projects\\docker_poc&gt; docker run -p 4000:4000 simplebackend\n\n&gt; backend@1.0.0 start\n&gt; nodemon --exec babel-node index.js\n\n[nodemon] 2.0.19\n[nodemon] to restart at any time, enter `rs`\n[nodemon] watching path(s): *.*\n[nodemon] watching extensions: js,mjs,json\n[nodemon] starting `babel-node index.js`\nYour server is running on PORT: 4000\n</code></pre></li> </ul> </li> <li>Step 5 - Run the following docker command to check if the container is up and running as a service<ul> <li>The container running can also be checked from the docker desktop as well as the number of images that have been created.</li> <li>Make sure that the container that you are running you shut down using the stop command</li> <li>Also, keep a check on the number of images that have been created. These images general run into 1-2 GBs and will very quickly eat up your local disk space.</li> </ul> </li> </ul> <pre><code>PS C:\\Users\\abhis\\Projects\\docker_poc&gt; docker ps\nCONTAINER ID   IMAGE           COMMAND                  CREATED          STATUS          PORTS                    NAMES\ncd637f86b4a8   simplebackend   \"docker-entrypoint.s\u2026\"   33 seconds ago   Up 32 seconds   0.0.0.0:4000-&gt;4000/tcp   great_heyrovsky\nPS C:\\Users\\abhis\\Projects\\docker_poc&gt; \nPS C:\\Users\\abhis\\Projects\\docker_poc&gt; docker stop cd637\ncd637\n</code></pre> <ul> <li>For running a full stack that comprise of frontend , backend, and a database consists of spinning up multiple containers that often form a stack. These containers can be combined to create a service that can be built suing docker compose</li> <li>To look at how to setup a multi-container based full stack service look at the notes Docker Compose</li> <li>To develop a CI pipeline from Github project using Travis CI app refer to the notes on CI with Travis<ul> <li>This takes it to full circle where once the code is developed, tested locally, and committed to the git repo in Github a continuous integration tool such as Travis can be used for continuous build that gets triggered for each commit to compile the code and push the image into a remote repository such as docker hub.</li> </ul> </li> </ul> <ul> <li>These notes are based on my understanding of the topic. Some of the content and images refers to the online articles, learning courses I attended. I encourage you to lookup the links provided to develop your own understanding of it at deeper level.</li> <li>The intention of these notes is to serve as reference marker to other sites, online courses, books that I found useful while learning about it.</li> <li>If you would like to share any thoughts or give comments please feel free to reach me @ abhishekgupta86@gmail.com or @ www.linkedin.com/in/abhishekgupta86</li> </ul>"},{"location":"Cloud/Chapter-Azure/Azure/","title":"Azure","text":"<ol> <li>Introduce to Azure compute.</li> </ol>"},{"location":"Data%20Management/Chapter-Data%20Management/Snowflake%20versus%20Databricks/","title":"Snowflake Vs. Databricks","text":""},{"location":"Data%20Management/Chapter-Data%20Management/Snowflake%20versus%20Databricks/#snowflake-vs-databricks","title":"Snowflake Vs. Databricks","text":"<p>Created: 2022-06-28 09:47:11 -0400</p> <p>Modified: 2022-06-29 07:54:58 -0400</p> <p></p> Feature Databricks Snowflake Founders <ul> <li><p>Ex-Berkley, Authors of Spark. Donated Spark to Apache and then opened managed services company as Databricks.</p></li> <li><p>Planned for IPO in 2022</p></li> </ul> <ul> <li><p>Ex-Oracle, Oracle principles behind the snowflake architecture</p></li> <li><p>Public listed. Revenue $1.2B</p></li> </ul> Architecture Big data distributed computing provided spark as a managed service. SaaS Datawarehouse platform. Truly separates storage from compute. Implementation Cloud Tight with Azure but also available on GCP, AWS Tight with AWS but also available on GCP, Azure Machine Learning <ul> <li><p>ML Lib library. Native to the Spark distribution.</p></li> <li><p>ML Core is the core library that does internal management, scheduling, memory management.</p></li> <li><p>Jupyter Notebook style environment to perform the ML experiments.</p></li> </ul> <p></p> <ul> <li><p>Provides machine learning capability through newly launched feature Snowpark.</p></li> <li><p>It is Snowflake + Spark providing connectors through java, scala, python api.</p></li> <li><p>It requires the data to be available in the snowflake. Else you need to load the data from lake into snowflake.</p></li> <li><p>Partner ecosystem that integrates with snowflake.</p></li> </ul> Data Ingestion <ul> <li><p>ML Streams. Part of the Spark core distribution.</p></li> <li><p>Provides both real-time events and batch ELT processing.</p></li> <li><p>Loads the data from various data sources including data lake.</p></li> </ul> <ul> <li><p>Uses partner network to provide the ingestion.</p></li> <li><p>Databricks is also a partner of snowflake</p></li> <li><p>Has developed snowpipe for short batch mode data ingestion</p></li> </ul> Use cases <ul> <li><p>Focused on Machine learning use cases</p></li> <li><p>Big data processing</p></li> </ul> <ul> <li><p>Focused on data warehousing use cases</p></li> <li><p>Developing dashboards on top of processed data from snowflake</p></li> </ul> Pricing <ul> <li><p>Pay as you go pricing. You pay for the databricks and the underlying resources such as the compute and storage used in the cloud</p></li> </ul> <ul> <li><p>Pay as you go. You pay for the storage of the data and the compute. This is in form a warehouse servers which can be spin up to perform the compute.</p></li> </ul> Scalability <ul> <li><p>Clusters can be scaled with 100's of spark executor nodes added to the cluster to perform the compute.</p></li> <li><p>Clusters can be spin on for each job to be executed and then shut down the job completes.</p></li> <li><p>Cluster can also be spin up for performing ML development</p></li> <li><p>Has auto scaling mode.</p></li> </ul> <ul> <li><p>Warehouse machines are small, medium, large, X Large. Each has defined number of nodes that can be increased or decreased in auto-scaling mode.</p></li> </ul> Users <ul> <li><p>Data Engineers, Data Scientist, Advanced Data analysts</p></li> </ul> <ul> <li><p>Data analysts, business users, Data engineers</p></li> </ul> Coding languages <ul> <li><p>Java, Scala (Major), Pyspark(Major), C, Hive SQL</p></li> </ul> <ul> <li><p>ANSI SQL, Python (Snowpark. New feature), Scala (Snowpark. New feature)</p></li> </ul>"},{"location":"Healthcare/Prior%20Authorization%20Interoperability%20CME%20Rule/","title":"Prior Authorization Interoperability CME Rule","text":"<ol> <li>"},{"location":"IT%20Architecture/Digital%20Signatures/","title":"Digital Signatures","text":"<ol> <li>What is digital signatures</li> </ol>"},{"location":"Leadership/10%20Key%20Qualities%20of%20a%20Successful%20Manager/","title":"10 Key Qualities of a Successful Manager","text":"<ol> <li>"},{"location":"Software%20Tools/Git/","title":"Git","text":"<ol> <li>What the top 10 git commands</li> </ol>"}]}